{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31c5c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sno                 sname  marks       city      mobile  gender\n",
      "0     1        gunja srikanth   50.0  hyderabad  9398968719    male\n",
      "1     2        dasari supriya   78.0  hyderabad  9398968719  female\n",
      "2     3       varkala tejasri   10.0  hyderabad  9398968719  female\n",
      "3     4     lavoori varshitha   54.0  hyderabad  9398968719  female\n",
      "4     5       edunuri bhavika   95.0  hyderabad  9398968719  female\n",
      "5     6  bejagam chathan teja   87.0  hyderabad  9398968719    male\n",
      "6     7             c pradeep   82.0  hyderabad  9398968719    male\n",
      "7     8   rachapally nikhitha   87.0       bglr  9398968719  female\n",
      "8     9      mandala nikhitha   82.0  hyderabad  9398968719  female\n",
      "9    10     battula ram gopal   90.0  hyderabad  9398968719    male\n",
      "10   11        bhukya sridhar   77.0       pune  9398968719    male\n",
      "11   12           shiva kumar   75.0       pune  9398968719    male\n",
      "12   13               mounika   75.0       pune  9398968719  female\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    " \n",
    "# Replace the following with your database connection details\n",
    "\n",
    "username = 'root'         # Your MySQL username\n",
    "\n",
    "password = '1234'         # Your MySQL password\n",
    "\n",
    "host = 'localhost'        # Your MySQL host, e.g., 'localhost' or an IP address\n",
    "\n",
    "database = 'cvr'          # Your MySQL database name\n",
    " \n",
    "# Create a connection string using SQLAlchemy\n",
    "\n",
    "connection_string = f'mysql+mysqlconnector://{username}:{password}@{host}/{database}'\n",
    " \n",
    "# Create an engine\n",
    "\n",
    "engine = create_engine(connection_string)\n",
    " \n",
    "# Read data from MySQL into a pandas DataFrame\n",
    "\n",
    "df_sql = pd.read_sql('SELECT * FROM students', engine)\n",
    " \n",
    "# Display the DataFrame\n",
    "\n",
    "print(df_sql)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e19ebdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "Memory Usage: Index        132\n",
      "order_id    4000\n",
      "dtype: int64\n",
      "Memory Usage: Index        132\n",
      "order_id    2000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "# Creating a sample dataset\n",
    "\n",
    "data = {'order_id': np.arange(1, 1001)}  # order_id from 1 to 1000\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "# Check the default data type\n",
    "\n",
    "print(df['order_id'].dtype)  # This will show int64\n",
    "\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True)}\")\n",
    "\n",
    "# Convert order_id to int16 (smallest type that fits)\n",
    "\n",
    "df['order_id'] = df['order_id'].astype(np.int16)\n",
    " \n",
    "# Check the memory usage\n",
    "\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True)}\")\n",
    " \n",
    "#Memory Saving: The int16 takes only 2 bytes per value, whereas int64 takes 8 bytes.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b79a7aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "Memory Usage: Index               132\n",
      "sales_amount    4000000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset with float values\n",
    "data = {'sales_amount': np.random.random(1000000)}  # Random float values\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "# Check the default data type\n",
    "print(df['sales_amount'].dtype)  # This will show float64\n",
    " \n",
    "# Convert to float32 for memory optimization\n",
    "df['sales_amount'] = df['sales_amount'].astype(np.float32)\n",
    " \n",
    "# Check the memory usage\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True)}\")\n",
    " \n",
    "#float32 takes 4 bytes compared to 8 bytes for float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "418f5e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "Memory Usage: Index       132\n",
      "city     500305\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset with repeated string values\n",
    "data = {'city': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Los Angeles'] * 100000}\n",
    "df = pd.DataFrame(data)\n",
    " \n",
    "# Check the memory usage with object type\n",
    "print(df['city'].dtype)  # This will show object (i.e., string)\n",
    " \n",
    "# Convert to category type\n",
    "df['city'] = df['city'].astype('category')\n",
    " \n",
    "# Check the memory usage after conversion\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Deletion\n",
    "\n",
    "#Listwise Deletion: Remove rows with any missing values.\n",
    "\n",
    "data_cleaned = data.dropna()\n",
    " \n",
    "#2. Imputation\n",
    "\n",
    "#Mean Imputation: Replace missing values with the column mean (numerical data)\n",
    "\n",
    "data['ColumnName'].fillna(data['ColumnName'].mean(), inplace=True)\n",
    " \n",
    "# Median Imputation: Use the column median\n",
    "\n",
    "data['ColumnName'].fillna(data['ColumnName'].median(), inplace=True)\n",
    " \n",
    "#Mode Imputation: Use the most frequent value (categorical data)\n",
    "\n",
    "data['ColumnName'].fillna(data['ColumnName'].mode()[0], inplace=True)\n",
    " \n",
    "# Check for duplicate rows\n",
    "\n",
    "duplicates = data.duplicated()\n",
    "\n",
    "print(f\"Number of duplicate rows: {duplicates.sum()}\")\n",
    " \n",
    "# Display duplicated rows\n",
    "\n",
    "print(data[data.duplicated()])\n",
    " \n",
    "#Remove all duplicate rows (full duplicates):\n",
    "\n",
    "data_cleaned = data.drop_duplicates()\n",
    " \n",
    "#Remove duplicates based on specific columns (partial duplicates):\n",
    "\n",
    "data_cleaned = data.drop_duplicates(subset=['Name', 'Age'])\n",
    " \n",
    "#Keeping the First/Last Occurrence\n",
    "\n",
    "#You can choose to keep either the first or last occurrence of a duplicate row.\n",
    " \n",
    "# Keep the first occurrence of each duplicate\n",
    "\n",
    "data_cleaned = data.drop_duplicates(keep='first')\n",
    " \n",
    "# Keep the last occurrence\n",
    "\n",
    "data_cleaned = data.drop_duplicates(keep='last')\n",
    "\n",
    " \n",
    "# Replace typos in 'City' column\n",
    "\n",
    "data['City'] = data['City'].replace({'Nwe York': 'New York', 'Los Angles': 'Los Angeles'})\n",
    "\n",
    " \n",
    "# Convert all text in 'City' column to lowercase\n",
    "\n",
    "data['City'] = data['City'].str.lower()\n",
    "\n",
    " \n",
    "Remove extra spaces:\n",
    "\n",
    "data['City'] = data['City'].str.strip()\n",
    "\n",
    " \n",
    "Handle inconsistent delimiters in categorical data:\n",
    "\n",
    "data['Education'] = data['Education'].str.replace(';', ',')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf910733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>START_DATE*</th>\n",
       "      <th>END_DATE*</th>\n",
       "      <th>CATEGORY*</th>\n",
       "      <th>START*</th>\n",
       "      <th>STOP*</th>\n",
       "      <th>MILES*</th>\n",
       "      <th>PURPOSE*</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2016 21:11</td>\n",
       "      <td>1/1/2016 21:17</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Meal/Entertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/2/2016 1:25</td>\n",
       "      <td>1/2/2016 1:37</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/2/2016 20:25</td>\n",
       "      <td>1/2/2016 20:38</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Errand/Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/5/2016 17:31</td>\n",
       "      <td>1/5/2016 17:45</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/6/2016 14:42</td>\n",
       "      <td>1/6/2016 15:49</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>West Palm Beach</td>\n",
       "      <td>63.7</td>\n",
       "      <td>Customer Visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>12/31/2016 13:24</td>\n",
       "      <td>12/31/2016 13:42</td>\n",
       "      <td>Business</td>\n",
       "      <td>Kar?chi</td>\n",
       "      <td>Unknown Location</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Temporary Site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>12/31/2016 15:03</td>\n",
       "      <td>12/31/2016 15:38</td>\n",
       "      <td>Business</td>\n",
       "      <td>Unknown Location</td>\n",
       "      <td>Unknown Location</td>\n",
       "      <td>16.2</td>\n",
       "      <td>Meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>12/31/2016 21:32</td>\n",
       "      <td>12/31/2016 21:50</td>\n",
       "      <td>Business</td>\n",
       "      <td>Katunayake</td>\n",
       "      <td>Gampaha</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Temporary Site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>12/31/2016 22:08</td>\n",
       "      <td>12/31/2016 23:51</td>\n",
       "      <td>Business</td>\n",
       "      <td>Gampaha</td>\n",
       "      <td>Ilukwatta</td>\n",
       "      <td>48.2</td>\n",
       "      <td>Temporary Site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>Totals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12204.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1156 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           START_DATE*         END_DATE* CATEGORY*            START*  \\\n",
       "0       1/1/2016 21:11    1/1/2016 21:17  Business       Fort Pierce   \n",
       "1        1/2/2016 1:25     1/2/2016 1:37  Business       Fort Pierce   \n",
       "2       1/2/2016 20:25    1/2/2016 20:38  Business       Fort Pierce   \n",
       "3       1/5/2016 17:31    1/5/2016 17:45  Business       Fort Pierce   \n",
       "4       1/6/2016 14:42    1/6/2016 15:49  Business       Fort Pierce   \n",
       "...                ...               ...       ...               ...   \n",
       "1151  12/31/2016 13:24  12/31/2016 13:42  Business           Kar?chi   \n",
       "1152  12/31/2016 15:03  12/31/2016 15:38  Business  Unknown Location   \n",
       "1153  12/31/2016 21:32  12/31/2016 21:50  Business        Katunayake   \n",
       "1154  12/31/2016 22:08  12/31/2016 23:51  Business           Gampaha   \n",
       "1155            Totals               NaN       NaN               NaN   \n",
       "\n",
       "                 STOP*   MILES*         PURPOSE*  \n",
       "0          Fort Pierce      5.1   Meal/Entertain  \n",
       "1          Fort Pierce      5.0              NaN  \n",
       "2          Fort Pierce      4.8  Errand/Supplies  \n",
       "3          Fort Pierce      4.7          Meeting  \n",
       "4      West Palm Beach     63.7   Customer Visit  \n",
       "...                ...      ...              ...  \n",
       "1151  Unknown Location      3.9   Temporary Site  \n",
       "1152  Unknown Location     16.2          Meeting  \n",
       "1153           Gampaha      6.4   Temporary Site  \n",
       "1154         Ilukwatta     48.2   Temporary Site  \n",
       "1155               NaN  12204.7              NaN  \n",
       "\n",
       "[1156 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"C:\\\\Users\\\\CVR\\\\Downloads\\\\Uber.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae82b2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>START_DATE*</th>\n",
       "      <th>END_DATE*</th>\n",
       "      <th>CATEGORY*</th>\n",
       "      <th>START*</th>\n",
       "      <th>STOP*</th>\n",
       "      <th>MILES*</th>\n",
       "      <th>PURPOSE*</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2016 21:11</td>\n",
       "      <td>1/1/2016 21:17</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>5.1</td>\n",
       "      <td>Meal/Entertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/2/2016 20:25</td>\n",
       "      <td>1/2/2016 20:38</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Errand/Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/5/2016 17:31</td>\n",
       "      <td>1/5/2016 17:45</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/6/2016 14:42</td>\n",
       "      <td>1/6/2016 15:49</td>\n",
       "      <td>Business</td>\n",
       "      <td>Fort Pierce</td>\n",
       "      <td>West Palm Beach</td>\n",
       "      <td>63.7</td>\n",
       "      <td>Customer Visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/6/2016 17:15</td>\n",
       "      <td>1/6/2016 17:19</td>\n",
       "      <td>Business</td>\n",
       "      <td>West Palm Beach</td>\n",
       "      <td>West Palm Beach</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Meal/Entertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>12/31/2016 1:07</td>\n",
       "      <td>12/31/2016 1:14</td>\n",
       "      <td>Business</td>\n",
       "      <td>Kar?chi</td>\n",
       "      <td>Kar?chi</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>12/31/2016 13:24</td>\n",
       "      <td>12/31/2016 13:42</td>\n",
       "      <td>Business</td>\n",
       "      <td>Kar?chi</td>\n",
       "      <td>Unknown Location</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Temporary Site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>12/31/2016 15:03</td>\n",
       "      <td>12/31/2016 15:38</td>\n",
       "      <td>Business</td>\n",
       "      <td>Unknown Location</td>\n",
       "      <td>Unknown Location</td>\n",
       "      <td>16.2</td>\n",
       "      <td>Meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>12/31/2016 21:32</td>\n",
       "      <td>12/31/2016 21:50</td>\n",
       "      <td>Business</td>\n",
       "      <td>Katunayake</td>\n",
       "      <td>Gampaha</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Temporary Site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>12/31/2016 22:08</td>\n",
       "      <td>12/31/2016 23:51</td>\n",
       "      <td>Business</td>\n",
       "      <td>Gampaha</td>\n",
       "      <td>Ilukwatta</td>\n",
       "      <td>48.2</td>\n",
       "      <td>Temporary Site</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>653 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           START_DATE*         END_DATE* CATEGORY*            START*  \\\n",
       "0       1/1/2016 21:11    1/1/2016 21:17  Business       Fort Pierce   \n",
       "2       1/2/2016 20:25    1/2/2016 20:38  Business       Fort Pierce   \n",
       "3       1/5/2016 17:31    1/5/2016 17:45  Business       Fort Pierce   \n",
       "4       1/6/2016 14:42    1/6/2016 15:49  Business       Fort Pierce   \n",
       "5       1/6/2016 17:15    1/6/2016 17:19  Business   West Palm Beach   \n",
       "...                ...               ...       ...               ...   \n",
       "1150   12/31/2016 1:07   12/31/2016 1:14  Business           Kar?chi   \n",
       "1151  12/31/2016 13:24  12/31/2016 13:42  Business           Kar?chi   \n",
       "1152  12/31/2016 15:03  12/31/2016 15:38  Business  Unknown Location   \n",
       "1153  12/31/2016 21:32  12/31/2016 21:50  Business        Katunayake   \n",
       "1154  12/31/2016 22:08  12/31/2016 23:51  Business           Gampaha   \n",
       "\n",
       "                 STOP*  MILES*         PURPOSE*  \n",
       "0          Fort Pierce     5.1   Meal/Entertain  \n",
       "2          Fort Pierce     4.8  Errand/Supplies  \n",
       "3          Fort Pierce     4.7          Meeting  \n",
       "4      West Palm Beach    63.7   Customer Visit  \n",
       "5      West Palm Beach     4.3   Meal/Entertain  \n",
       "...                ...     ...              ...  \n",
       "1150           Kar?chi     0.7          Meeting  \n",
       "1151  Unknown Location     3.9   Temporary Site  \n",
       "1152  Unknown Location    16.2          Meeting  \n",
       "1153           Gampaha     6.4   Temporary Site  \n",
       "1154         Ilukwatta    48.2   Temporary Site  \n",
       "\n",
       "[653 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned=data.dropna()\n",
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81544ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after listwise deletion:\n",
      "      START_DATE*       END_DATE* CATEGORY*           START*            STOP*  \\\n",
      "0  1/1/2016 21:11  1/1/2016 21:17  Business      Fort Pierce      Fort Pierce   \n",
      "2  1/2/2016 20:25  1/2/2016 20:38  Business      Fort Pierce      Fort Pierce   \n",
      "3  1/5/2016 17:31  1/5/2016 17:45  Business      Fort Pierce      Fort Pierce   \n",
      "4  1/6/2016 14:42  1/6/2016 15:49  Business      Fort Pierce  West Palm Beach   \n",
      "5  1/6/2016 17:15  1/6/2016 17:19  Business  West Palm Beach  West Palm Beach   \n",
      "\n",
      "   MILES*         PURPOSE*  \n",
      "0     5.1   Meal/Entertain  \n",
      "2     4.8  Errand/Supplies  \n",
      "3     4.7          Meeting  \n",
      "4    63.7   Customer Visit  \n",
      "5     4.3   Meal/Entertain  \n",
      "Number of duplicate rows: 1\n",
      "Duplicate rows:\n",
      "         START_DATE*        END_DATE* CATEGORY*  START* STOP*  MILES* PURPOSE*\n",
      "492  6/28/2016 23:34  6/28/2016 23:59  Business  Durham  Cary     9.9  Meeting\n",
      "Cleaned data:\n",
      "      START_DATE*       END_DATE* CATEGORY*       START*            STOP*  \\\n",
      "0  1/1/2016 21:11  1/1/2016 21:17  Business  Fort Pierce      Fort Pierce   \n",
      "1   1/2/2016 1:25   1/2/2016 1:37  Business  Fort Pierce      Fort Pierce   \n",
      "2  1/2/2016 20:25  1/2/2016 20:38  Business  Fort Pierce      Fort Pierce   \n",
      "3  1/5/2016 17:31  1/5/2016 17:45  Business  Fort Pierce      Fort Pierce   \n",
      "4  1/6/2016 14:42  1/6/2016 15:49  Business  Fort Pierce  West Palm Beach   \n",
      "\n",
      "   MILES*         PURPOSE*  \n",
      "0     5.1   Meal/Entertain  \n",
      "1     5.0          Meeting  \n",
      "2     4.8  Errand/Supplies  \n",
      "3     4.7          Meeting  \n",
      "4    63.7   Customer Visit  \n",
      "Cleaned data saved to Uber_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\CVR\\\\Downloads\\\\Uber.csv\")\n",
    "\n",
    "# 1. Deletion: Remove rows with any missing values\n",
    "data_cleaned = data.dropna()\n",
    "print(\"Data after listwise deletion:\")\n",
    "print(data_cleaned.head())\n",
    "\n",
    "# 2. Imputation\n",
    "# Example: Replace missing values in 'MILES*' with the mean\n",
    "if 'MILES*' in data.columns:\n",
    "    data['MILES*'] = data['MILES*'].astype(float)  # Ensure it's numeric\n",
    "    data['MILES*'].fillna(data['MILES*'].mean(), inplace=True)\n",
    "\n",
    "# Example: Replace missing values in 'PURPOSE*' with the mode\n",
    "if 'PURPOSE*' in data.columns:\n",
    "    data['PURPOSE*'].fillna(data['PURPOSE*'].mode()[0], inplace=True)\n",
    "\n",
    "# 3. Check for duplicates\n",
    "duplicates = data.duplicated()\n",
    "print(f\"Number of duplicate rows: {duplicates.sum()}\")\n",
    "\n",
    "# Display duplicate rows, if any\n",
    "if duplicates.sum() > 0:\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(data[duplicates])\n",
    "\n",
    "# Remove all duplicate rows\n",
    "data_cleaned = data.drop_duplicates()\n",
    "\n",
    "# 4. Remove duplicates based on specific columns (e.g., 'START*', 'STOP*')\n",
    "data_cleaned = data.drop_duplicates(subset=['START*', 'STOP*'])\n",
    "\n",
    "# 5. Keep the first occurrence of duplicates\n",
    "data_cleaned = data.drop_duplicates(keep='first')\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"Cleaned data:\")\n",
    "print(data_cleaned.head())\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "data_cleaned.to_csv(\"C:\\\\Users\\\\CVR\\\\Downloads\\\\Uber_cleaned.csv\", index=False)\n",
    "print(\"Cleaned data saved to Uber_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1c2a950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'retail_sales_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretail_sales_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 1. Handling Missing Values\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Identify missing values\u001b[39;00m\n\u001b[0;32m     10\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'retail_sales_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('retail_sales_dataset.csv')\n",
    "\n",
    "# 1. Handling Missing Values\n",
    "# Identify missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Fill missing values\n",
    "# Median for Total Amount\n",
    "df['Total Amount'].fillna(df['Total Amount'].median(), inplace=True)\n",
    "# Mode for Product Category and Customer ID\n",
    "df['Product Category'].fillna(df['Product Category'].mode()[0], inplace=True)\n",
    "df['Customer ID'].fillna(df['Customer ID'].mode()[0], inplace=True)\n",
    "\n",
    "# Drop rows with too many missing values (threshold: 50% missing)\n",
    "df = df.dropna(thresh=df.shape[1] * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015eaf49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
